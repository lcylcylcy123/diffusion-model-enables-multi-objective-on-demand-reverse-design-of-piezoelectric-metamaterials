{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d6ad0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/HOME/scz0ruj/.conda/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNDMScheduler {\n",
      "  \"_class_name\": \"PNDMScheduler\",\n",
      "  \"_diffusers_version\": \"0.12.1\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": false,\n",
      "  \"skip_prk_steps\": true,\n",
      "  \"steps_offset\": 1,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "from diffusers import PNDMScheduler\n",
    "\n",
    "# 创建 PNDMScheduler 实例\n",
    "scheduler = PNDMScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    num_train_timesteps=1000,\n",
    "    prediction_type=\"epsilon\",\n",
    "    set_alpha_to_one=False,\n",
    "    skip_prk_steps=True,\n",
    "    steps_offset=1,\n",
    "    trained_betas=None\n",
    ")\n",
    "\n",
    "# 输出scheduler的配置，确保正确设置\n",
    "print(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0667cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.84952491e+02 2.43981134e+02 1.12512536e+00 1.84848683e-01]\n",
      "[1.19021432e+02 1.23790469e+02 1.13191379e+00 2.54632684e-02]\n",
      "15959\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 图片和数据文件的路径\n",
    "image_dir = r\"/data/run01/scz0ruj/picturedata1/\"\n",
    "data_file = r\"/data/run01/scz0ruj/reduced_data_output.xlsx\"\n",
    "\n",
    "# 读取 Excel 文件\n",
    "data_df = pd.read_excel(data_file, nrows=16000)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_dir, dataframe):\n",
    "        self.images_dir = images_dir\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        # 计算条件数据的均值和标准差\n",
    "        condition_data = dataframe.iloc[:, 1:5]  # 假设条件数据位于第二列到第五列\n",
    "        self.condition_mean = condition_data.mean().values\n",
    "        self.condition_std = condition_data.std().values\n",
    "        print(self.condition_mean)\n",
    "        print(self.condition_std)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = f\"lcy{self.dataframe.iloc[idx, 0]}.jpg\"\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        conditions = torch.tensor(self.dataframe.iloc[idx, 1:5].values, dtype=torch.float)\n",
    "        # 应用条件数据的归一化\n",
    "        conditions = (conditions - torch.tensor(self.condition_mean)) / torch.tensor(self.condition_std)\n",
    "        # 使用浮点类型初始化向量，长度为77\n",
    "        zeroconditions = torch.zeros(4, dtype=torch.float)\n",
    "\n",
    "        # 将包含四个数字的列表转换为浮点型张量，并赋值给向量的前四个位置\n",
    "        zeroconditions[:4] = torch.tensor(conditions, dtype=torch.float)\n",
    "        \n",
    "        return {'pixel_values': image, 'input_ids': zeroconditions}\n",
    "\n",
    "# 创建数据集实例\n",
    "dataset = CustomDataset(image_dir, data_df)\n",
    "print(len(dataset))\n",
    "\n",
    "# 创建数据集实例\n",
    "#dataset = CustomDataset(image_dir, data_df)\n",
    "\n",
    "\n",
    "# 创建 DataLoader\n",
    "#dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "# 获取并打印第一批数据的形状\n",
    "#for batch in dataloader:\n",
    "#    pixel_values = batch['pixel_values']\n",
    "#    input_ids = batch['input_ids']\n",
    "#    print(\"Batch pixel_values shape:\", pixel_values.shape)\n",
    "#    print(\"Batch input_ids shape:\", input_ids)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5746162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29735/3263394976.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  zeroconditions[:4] = torch.tensor(conditions, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch pixel values shape: torch.Size([32, 3, 256, 256])\n",
      "First batch input ids shape: torch.Size([32, 4])\n",
      "First batch input ids shape: tensor([[-5.0728e-01, -1.1266e+00, -8.0486e-01, -6.4399e-01],\n",
      "        [ 1.4553e+00,  1.6179e+00,  1.8313e+00,  7.7973e-01],\n",
      "        [-9.4184e-02,  1.0695e-01, -2.5038e-01,  2.8159e-01],\n",
      "        [-8.6248e-02,  3.6250e-01, -1.8620e-01,  9.7174e-01],\n",
      "        [-5.7137e-02,  3.8759e-01, -2.3758e-01,  1.2371e+00],\n",
      "        [-3.6755e-02,  4.7165e-04, -4.3205e-01,  3.0025e-01],\n",
      "        [-6.1543e-02,  2.9760e-01, -7.3897e-02,  2.1439e-01],\n",
      "        [-3.3004e-01,  1.9199e-01, -4.2557e-01,  3.4050e-01],\n",
      "        [-4.5301e-01, -3.6887e-01, -4.7121e-01, -1.7193e-01],\n",
      "        [-1.3936e+00, -1.4224e+00, -8.3864e-01, -1.3361e+00],\n",
      "        [-8.8614e-01, -6.1807e-01, -5.8472e-01,  1.4315e-01],\n",
      "        [-3.4177e-01, -3.0577e-02, -2.8006e-01,  7.7951e-01],\n",
      "        [-1.9946e-01, -6.3574e-01, -6.5613e-01, -1.0243e-01],\n",
      "        [ 3.0547e-01,  4.5344e-01, -1.5514e-01,  6.5678e-01],\n",
      "        [ 3.5717e-02, -1.5353e-01, -4.5508e-01, -3.2064e-02],\n",
      "        [-1.2548e+00, -1.2880e+00, -8.0730e-01, -8.2651e-01],\n",
      "        [-1.6995e+00, -1.3193e+00, -8.3538e-01, -1.0851e+00],\n",
      "        [-6.1093e-01, -5.6376e-01, -6.0797e-01, -4.1663e-01],\n",
      "        [ 6.3407e-01,  6.0851e-01, -8.4131e-03,  4.8506e-01],\n",
      "        [ 1.4887e+00,  1.5335e+00,  2.1398e+00,  1.3107e+00],\n",
      "        [-1.3759e-01, -7.0383e-01, -6.6257e-01, -7.2745e-01],\n",
      "        [ 1.7035e+00,  2.0987e+00,  2.9943e+00,  1.8061e+00],\n",
      "        [ 3.5776e-01,  2.9570e-01, -6.5049e-02,  3.8021e-01],\n",
      "        [-1.2290e+00, -1.0449e+00, -8.0853e-01, -9.6463e-01],\n",
      "        [-1.2906e+00, -1.6427e+00, -8.5509e-01, -1.5564e+00],\n",
      "        [-1.2448e+00, -8.7474e-01, -7.2867e-01, -1.1578e+00],\n",
      "        [ 1.2691e+00,  1.2704e+00,  1.1656e+00,  7.2082e-01],\n",
      "        [ 1.5256e+00,  1.7886e+00,  2.7070e+00,  1.2627e+00],\n",
      "        [-4.4855e-01, -3.1844e-01, -4.8818e-01,  3.6106e-03],\n",
      "        [-1.3166e+00, -1.1024e+00, -7.6487e-01, -8.9764e-01],\n",
      "        [-5.5774e-02, -1.0068e-01, -3.5861e-01,  2.6141e-01],\n",
      "        [-1.9258e+00, -1.2951e+00, -8.1917e-01, -9.4663e-01]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# 定义 collate_fn 函数\n",
    "def collate_fn(data):\n",
    "    pixel_values = [i['pixel_values'] for i in data]\n",
    "    input_ids = [i['input_ids'] for i in data]\n",
    "\n",
    "    # 将列表的数据堆叠成一个新的Tensor，并转移到设备上\n",
    "    pixel_values = torch.stack(pixel_values).to(device)\n",
    "    input_ids = torch.stack(input_ids).to(device)\n",
    "    \n",
    "    return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "\n",
    "# 创建 DataLoader 实例\n",
    "loader = DataLoader(dataset,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=collate_fn,\n",
    "                    batch_size=32,\n",
    "                    num_workers=0)  # 可以根据实际情况调整 num_workers\n",
    "\n",
    "\n",
    "# 获取 DataLoader 的长度，这是根据数据集的大小和批次大小计算得出的总批次数量\n",
    "print(\"Total batches:\", len(loader))\n",
    "\n",
    "# 使用 next(iter(loader)) 获取第一个批次的数据\n",
    "first_batch = next(iter(loader))\n",
    "print(\"First batch pixel values shape:\", first_batch['pixel_values'].shape)\n",
    "print(\"First batch input ids shape:\", first_batch['input_ids'].shape)\n",
    "print(\"First batch input ids shape:\", first_batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1370f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AdamW (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     lr: 0.0001\n",
       "     maximize: False\n",
       "     weight_decay: 0.1\n",
       " ),\n",
       " MSELoss())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载模型\n",
    "%run 2.vae.ipynb\n",
    "%run 3.Unet.ipynb\n",
    "\n",
    "#准备训练\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(True)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "vae.eval()\n",
    "unet.train()\n",
    "\n",
    "\n",
    "vae.to(device)\n",
    "unet.to(device)\n",
    "\n",
    "#encoder.to(device)\n",
    "#vae.to(device)\n",
    "#unet.to(device)\n",
    "all_parameters = list(unet.parameters())\n",
    "optimizer = torch.optim.AdamW(all_parameters,\n",
    "                              lr=1e-4,\n",
    "                              betas=(0.9, 0.999),\n",
    "                              weight_decay=0.1,\n",
    "                              eps=1e-8)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad5447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(data):\n",
    "    input_ids = data['input_ids']\n",
    "    expanded_input_ids = input_ids.unsqueeze(-1).repeat(1, 1, 768)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        #抽取图像特征图\n",
    "        #[1, 3, 512, 512] -> [1, 4, 64, 64]\n",
    "        out_vae = vae.encoder(data['pixel_values'])\n",
    "        h,mean,logvar = vae.sample(out_vae)\n",
    "        # 假设out_vae的第一个元素是你需要的张量\n",
    "        out_vae = h  # 选择第一个元素，这应该是一个张量\n",
    "\n",
    "\n",
    "        #0.18215 = vae.config.scaling_factor\n",
    "        out_vae = out_vae * 0.18215\n",
    "\n",
    "    #随机数,unet的计算目标\n",
    "    noise = torch.randn_like(out_vae)\n",
    "    noise.to(device)\n",
    "\n",
    "    #往特征图中添加噪声\n",
    "    #1000 = scheduler.num_train_timesteps\n",
    "    #1 = batch size\n",
    "    noise_step = torch.randint(0, 1000, (1, )).long().to(device)\n",
    "    out_vae_noise = scheduler.add_noise(out_vae, noise, noise_step)\n",
    "\n",
    "    #根据文字信息,把特征图中的噪声计算出来\n",
    "    out_encoder = expanded_input_ids\n",
    "    out_unet = unet(out_vae=out_vae_noise,\n",
    "                    out_encoder=out_encoder,\n",
    "                    time=noise_step)\n",
    "\n",
    "    #计算mse loss\n",
    "    #[1, 4, 64, 64],[1, 4, 64, 64]\n",
    "    return criterion(out_unet, noise)\n",
    "\n",
    "\n",
    "## get_loss({\n",
    "##     'input_ids': torch.ones(1, 77, device=device).long(),\n",
    "##     'pixel_values': torch.randn(1, 3, 512, 512, device=device)\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ddadc9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29735/3263394976.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  zeroconditions[:4] = torch.tensor(conditions, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 46.989921575877815\n",
      "1 44.31479498371482\n",
      "2 35.63570671796333\n",
      "3 33.48140615748707\n",
      "4 41.79175331525039\n",
      "5 41.65768559387652\n",
      "6 33.60054513416253\n",
      "7 42.62671667133691\n",
      "8 39.92209193389863\n",
      "9 35.359812349663116\n",
      "10 36.191959898453206\n",
      "11 34.264407019363716\n",
      "12 37.67601086740615\n",
      "13 34.28923923627008\n",
      "14 41.027425063424744\n",
      "15 32.75533305306453\n",
      "16 35.92075070901774\n",
      "17 38.75645446800627\n",
      "18 33.80359188944567\n",
      "19 33.95740032414324\n",
      "20 36.30861529381946\n",
      "21 40.79492051945999\n",
      "22 34.418898941599764\n",
      "23 31.623108579864493\n",
      "24 37.77568921307102\n",
      "25 31.893371217505774\n",
      "26 31.93035977173713\n",
      "27 35.12924008499249\n",
      "28 32.43927613503183\n",
      "29 32.47857268858934\n",
      "30 32.08791530402959\n",
      "31 42.48215850981069\n",
      "32 34.43062313081464\n",
      "33 34.015066479565576\n",
      "34 32.3360882535635\n",
      "35 37.55802712656441\n",
      "36 38.10507327149389\n",
      "37 41.21301821930683\n",
      "38 31.858381258993177\n",
      "39 31.997658641601447\n",
      "40 33.75054135595565\n",
      "41 41.39536404219689\n",
      "42 32.05412896105554\n",
      "43 32.57255116890883\n",
      "44 37.86015860995394\n",
      "45 33.159710001869826\n",
      "46 34.73225807829294\n",
      "47 32.40540911821881\n",
      "48 45.33237561126589\n",
      "49 31.026774402853334\n",
      "50 36.9407088389853\n",
      "51 35.72068726454745\n",
      "52 32.971477676939685\n",
      "53 31.35704615022405\n",
      "54 36.86994879588019\n",
      "55 28.526769821561174\n",
      "56 35.79693063450395\n",
      "57 33.39516625300166\n",
      "58 34.386333985079546\n",
      "59 34.18235689625726\n",
      "60 33.852104271558346\n",
      "61 36.09039128085715\n",
      "62 39.022305934049655\n",
      "63 35.91061188819003\n",
      "64 37.78930789057631\n",
      "65 37.567411248688586\n",
      "66 30.9090623796219\n",
      "67 32.18379804052529\n",
      "68 34.11889262357727\n",
      "69 32.600190899247536\n",
      "70 33.45259788347175\n",
      "71 34.99176703940611\n",
      "72 36.02524089772487\n",
      "73 36.350008966604946\n",
      "74 38.19669641900691\n",
      "75 32.98172406727099\n",
      "76 36.264854884153465\n",
      "77 34.16685403345036\n",
      "78 32.053027026646305\n",
      "79 36.00175762784784\n",
      "80 28.400328210700536\n",
      "81 31.74919524768484\n",
      "82 36.32125525921583\n",
      "83 37.31979494358529\n",
      "84 35.66509433754254\n",
      "85 35.936514979519416\n",
      "86 30.21804218471516\n",
      "87 27.999607260280754\n",
      "88 31.604807360534323\n",
      "89 40.79343473148765\n",
      "90 31.43032221964677\n",
      "91 35.639720661740284\n",
      "92 36.345278511988\n",
      "93 35.948168263421394\n",
      "94 33.85967452748446\n",
      "95 33.587182829360245\n",
      "96 40.6065855906636\n",
      "97 40.91677971393801\n",
      "98 36.187211623211624\n",
      "99 26.442824478348484\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    loss_sum = 0\n",
    "    for epoch in range(100):\n",
    "        for i, data in enumerate(loader):\n",
    "            loss = get_loss(data)\n",
    "            loss.backward()\n",
    "            loss_sum += loss.item()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(all_parameters, 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(epoch, loss_sum)\n",
    "            loss_sum = 0\n",
    "\n",
    "    #torch.save(unet.to('cpu'), 'saves/unet.model')\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b09eb9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /data/run01/scz0ruj/model/lcynew_unet_model1.pth\n",
      "Model parameters saved to /data/run01/scz0ruj/model/lcynew_unet_model_parameters1.pth\n"
     ]
    }
   ],
   "source": [
    "# 保存整个模型\n",
    "model_path = \"/data/run01/scz0ruj/model/lcynew_unet_model1.pth\"  # 替换为你的文件路径\n",
    "torch.save(unet.to('cpu'), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# 仅保存模型参数\n",
    "parameters_path = \"/data/run01/scz0ruj/model/lcynew_unet_model_parameters1.pth\"  # 替换为你的文件路径\n",
    "torch.save(unet.to('cpu').state_dict(), parameters_path)\n",
    "print(f\"Model parameters saved to {parameters_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "398d6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存整个模型\n",
    "#model_path = \"/data/run01/scz0ruj/model/encoder_model2.pth\"  # 替换为你的文件路径\n",
    "#torch.save(encoder.to('cpu'), model_path)\n",
    "#print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# 仅保存模型参数\n",
    "#parameters_path = \"/data/run01/scz0ruj/model/lcynew_encoder_model_parameters.pth\"  # 替换为你的文件路径\n",
    "#torch.save(encoder.to('cpu').state_dict(), parameters_path)\n",
    "#print(f\"Model parameters saved to {parameters_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee0090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
