{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b82518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/HOME/scz0ruj/.conda/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aadb313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.s = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(num_groups=32,\n",
    "                               num_channels=dim_in,\n",
    "                               eps=1e-6,\n",
    "                               affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(dim_in,\n",
    "                            dim_out,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "            torch.nn.GroupNorm(num_groups=32,\n",
    "                               num_channels=dim_out,\n",
    "                               eps=1e-6,\n",
    "                               affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(dim_out,\n",
    "                            dim_out,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "        )\n",
    "\n",
    "        self.res = None\n",
    "        if dim_in != dim_out:\n",
    "            self.res = torch.nn.Conv2d(dim_in,\n",
    "                                       dim_out,\n",
    "                                       kernel_size=1,\n",
    "                                       stride=1,\n",
    "                                       padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x -> [1, 128, 10, 10]\n",
    "\n",
    "        res = x\n",
    "        if self.res:\n",
    "            #[1, 128, 10, 10] -> [1, 256, 10, 10]\n",
    "            res = self.res(x)\n",
    "\n",
    "        #[1, 128, 10, 10] -> [1, 256, 10, 10]\n",
    "        return res + self.s(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b316d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atten(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm = torch.nn.GroupNorm(num_channels=128,\n",
    "                                       num_groups=32,\n",
    "                                       eps=1e-6,\n",
    "                                       affine=True)\n",
    "\n",
    "        self.q = torch.nn.Linear(128, 128)\n",
    "        self.k = torch.nn.Linear(128, 128)\n",
    "        self.v = torch.nn.Linear(128, 128)\n",
    "        self.out = torch.nn.Linear(128, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x -> [1, 512, 64, 64]\n",
    "        res = x\n",
    "\n",
    "        #norm,维度不变\n",
    "        #[1, 512, 64, 64]\n",
    "        x = self.norm(x)\n",
    "\n",
    "        #[1, 512, 64, 64] -> [1, 512, 4096] -> [1, 4096, 512]\n",
    "        x = x.flatten(start_dim=2).transpose(1, 2)\n",
    "\n",
    "        #线性运算,维度不变\n",
    "        #[1, 4096, 512]\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        #[1, 4096, 512] -> [1, 512, 4096]\n",
    "        k = k.transpose(1, 2)\n",
    "\n",
    "        #[1, 4096, 512] * [1, 512, 4096] -> [1, 4096, 4096]\n",
    "        #0.044194173824159216 = 1 / 512**0.5\n",
    "        #atten = q.bmm(k) * 0.044194173824159216\n",
    "\n",
    "        #照理来说应该是等价的,但是却有很小的误差\n",
    "        atten = torch.baddbmm(torch.empty(1, 1024, 1024, device=q.device),\n",
    "                              q,\n",
    "                              k,\n",
    "                              beta=0,\n",
    "                              alpha=0.044194173824159216)\n",
    "\n",
    "        atten = torch.softmax(atten, dim=2)\n",
    "\n",
    "        #[1, 4096, 4096] * [1, 4096, 512] -> [1, 4096, 512]\n",
    "        atten = atten.bmm(v)\n",
    "\n",
    "        #线性运算,维度不变\n",
    "        #[1, 4096, 512]\n",
    "        atten = self.out(atten)\n",
    "\n",
    "        #[1, 4096, 512] -> [1, 512, 4096] -> [1, 512, 64, 64]\n",
    "        atten = atten.transpose(1, 2).reshape(-1, 128, 32, 32)\n",
    "\n",
    "        #残差连接,维度不变\n",
    "        #[1, 512, 64, 64]\n",
    "        atten = atten + res\n",
    "\n",
    "        return atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c1ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pad(torch.nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.pad(x, (0, 1, 0, 1),\n",
    "                                       mode='constant',\n",
    "                                       value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a13d8814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 256, 256]),\n",
       " torch.Size([1, 4, 32, 32]),\n",
       " torch.Size([1, 4, 32, 32]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            #in\n",
    "            torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            #down\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(32, 32),\n",
    "                Resnet(32, 32),\n",
    "                torch.nn.Sequential(\n",
    "                    Pad(),\n",
    "                    torch.nn.Conv2d(32, 32, 3, stride=2, padding=0),\n",
    "                ),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(32, 64),\n",
    "                Resnet(64, 64),\n",
    "                torch.nn.Sequential(\n",
    "                    Pad(),\n",
    "                    torch.nn.Conv2d(64,64, 3, stride=2, padding=0),\n",
    "                ),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(64, 128),\n",
    "                Resnet(128, 128),\n",
    "                torch.nn.Sequential(\n",
    "                    Pad(),\n",
    "                    torch.nn.Conv2d(128, 128, 3, stride=2, padding=0),\n",
    "                ),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(128, 128),\n",
    "                Resnet(128, 128),\n",
    "            ),\n",
    "\n",
    "            #mid\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(128, 128),\n",
    "                Atten(),\n",
    "                Resnet(128, 128),\n",
    "            ),\n",
    "\n",
    "            #out\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.GroupNorm(num_channels=128, num_groups=32, eps=1e-6),\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Conv2d(128, 8, 3, padding=1),\n",
    "            ),\n",
    "\n",
    "            #正态分布层\n",
    "            torch.nn.Conv2d(8, 8, 1),\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            #正态分布层\n",
    "            torch.nn.Conv2d(4, 4, 1),\n",
    "\n",
    "            #in\n",
    "            torch.nn.Conv2d(4, 128, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            #middle\n",
    "            torch.nn.Sequential(Resnet(128, 128), Atten(), Resnet(128,128 )),\n",
    "\n",
    "            #up\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(128, 128),\n",
    "                Resnet(128, 128),\n",
    "                torch.nn.Upsample(scale_factor=2.0, mode='nearest'),\n",
    "                torch.nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(128, 128),\n",
    "                Resnet(128, 128),\n",
    "                torch.nn.Upsample(scale_factor=2.0, mode='nearest'),\n",
    "                torch.nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(128, 64),\n",
    "                Resnet(64, 64),\n",
    "                torch.nn.Upsample(scale_factor=2.0, mode='nearest'),\n",
    "                torch.nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(64, 32),\n",
    "                Resnet(32, 32),\n",
    "            ),\n",
    "\n",
    "            #out\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.GroupNorm(num_channels=32, num_groups=32, eps=1e-6),\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Conv2d(32, 3, 3, padding=1),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def sample(self, h):\n",
    "        #h -> [1, 8, 64, 64]\n",
    "\n",
    "        #[1, 4, 64, 64]\n",
    "        mean = h[:, :4]\n",
    "        logvar = h[:, 4:]\n",
    "        std = logvar.exp()**0.5\n",
    "\n",
    "        #[1, 4, 64, 64]\n",
    "        h = torch.randn(mean.shape, device=mean.device)\n",
    "        h = mean + std * h\n",
    "\n",
    "        return h,mean,logvar\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x -> [1, 3, 512, 512]\n",
    "\n",
    "        #[1, 3, 512, 512] -> [1, 8, 64, 64]\n",
    "        h = self.encoder(x)\n",
    "\n",
    "        #[1, 8, 64, 64] -> [1, 4, 64, 64]\n",
    "        h,mu,log_var = self.sample(h)\n",
    "\n",
    "        #[1, 4, 64, 64] -> [1, 3, 512, 512]\n",
    "        h = self.decoder(h)\n",
    "\n",
    "        return h,mu,log_var\n",
    "pred, mu, log_var =VAE()(torch.randn(1, 3, 256, 256))\n",
    "pred.shape, mu.shape, log_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d81d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Sequential(\n",
       "      (0): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Pad()\n",
       "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (res): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Pad()\n",
       "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (res): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Pad()\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): Atten(\n",
       "        (norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (2): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (7): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Conv2d(4, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Sequential(\n",
       "      (0): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): Atten(\n",
       "        (norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (2): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (res): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (res): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): Resnet(\n",
       "        (s): Sequential(\n",
       "          (0): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (4): SiLU()\n",
       "          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型权重，并调整为单GPU或CPU格式\n",
    "def load_model(model, model_path):\n",
    "    # 加载保存的状态字典\n",
    "    state_dict = torch.load(model_path)\n",
    "\n",
    "    # 创建新的状态字典，移除'module.'前缀\n",
    "    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "    # 加载调整后的状态字典\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "# 加载模型并进行推断\n",
    "model_path = \"/data/run01/scz0ruj/model/best_model500.pth\"\n",
    "vae = VAE()\n",
    "load_model(vae, model_path)\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50722c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
